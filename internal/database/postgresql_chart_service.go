package database

import (
	"database/sql"
	"fmt"
	"time"
	"websocket-backend-new/internal/utils"
	"websocket-backend-new/models"
)

// PostgreSQLChartService provides ultra-fast chart data using materialized views
// This replaces the expensive real-time queries with pre-computed results
type PostgreSQLChartService struct {
	db *sql.DB
}

// NewPostgreSQLChartService creates a PostgreSQL-optimized chart service
func NewPostgreSQLChartService(db *sql.DB) *PostgreSQLChartService {
	return &PostgreSQLChartService{db: db}
}

// GetTransferRatesOptimized gets transfer rates from materialized view (FAST)
// Replaces: Multiple COUNT queries with 1 fast lookup
func (p *PostgreSQLChartService) GetTransferRatesOptimized() (FrontendTransferRates, error) {
	// Get the actual data timestamp range (not current time!)
	var totalRecords int64
	var minTimestamp, maxTimestamp int64
	var err error
	p.db.QueryRow("SELECT COUNT(*), MIN(timestamp), MAX(timestamp) FROM transfers").Scan(&totalRecords, &minTimestamp, &maxTimestamp)
	
	minTime := time.Unix(minTimestamp, 0)
	maxTime := time.Unix(maxTimestamp, 0)
	dataSpan := maxTime.Sub(minTime)
	
	utils.LogInfo("POSTGRESQL_CHART", "ðŸ“Š DATA ANALYSIS: %d total records | Span: %v (%0.1f hours) | From: %s | To: %s", 
		totalRecords, dataSpan, dataSpan.Hours(), minTime.Format("2006-01-02 15:04:05"), maxTime.Format("2006-01-02 15:04:05"))
	
	// Use the most recent transfer timestamp as our reference point (not current time!)
	dataEndTime := maxTime
	
	rates := FrontendTransferRates{
		LastUpdateTime: dataEndTime, // Use actual data timeline
		DataAvailability: DataAvailability{
			HasMinute: true,
			HasHour:   true,
			HasDay:    true,
			Has7Days:  true,
			Has14Days: true,
			Has30Days: true,
		},
		ServerUptimeSeconds: time.Since(dataEndTime.Add(-time.Hour)).Seconds(),
	}
	
	// Single fast query from materialized view for all periods
	// Calculate period counts using actual data timeline (not materialized views that might be outdated)
	periods := []string{"1m", "1h", "1d", "7d", "14d", "30d"}
	durations := map[string]time.Duration{
		"1m":  time.Minute,
		"1h":  time.Hour,
		"1d":  24 * time.Hour,
		"7d":  7 * 24 * time.Hour,
		"14d": 14 * 24 * time.Hour,
		"30d": 30 * 24 * time.Hour,
	}
	
	periodCounts := make(map[string]int64)
	for _, period := range periods {
		duration := durations[period]
		since := dataEndTime.Add(-duration)
		
		var currentCount int64
		err := p.db.QueryRow("SELECT COUNT(*) FROM transfers WHERE timestamp > $1", since.Unix()).Scan(&currentCount)
		if err != nil {
			currentCount = 0
		}
		periodCounts[period] = currentCount
		
		utils.LogInfo("POSTGRESQL_CHART", "ðŸ“ˆ Period %s: %d transfers from %s to %s", 
			period, currentCount, since.Format("2006-01-02 15:04:05"), dataEndTime.Format("2006-01-02 15:04:05"))
	}
	
	// Calculate current period counts and percentage changes
	now := time.Now()
	
	for _, period := range periods {
		currentCount := periodCounts[period]
		
		// Calculate previous period for percentage change using time-based comparison
		duration := durations[period]
		prevSince := now.Add(-2 * duration)
		prevUntil := now.Add(-duration)
		
		var prevCount int64
		var err error
		err = p.db.QueryRow("SELECT COUNT(*) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
			prevSince.Unix(), prevUntil.Unix()).Scan(&prevCount)
		if err != nil {
			prevCount = 0
		}
		
		// Calculate percentage change using improved logic
		var percentageChange float64
		if period == "1m" {
			// For minute data, compare against 5-minute average (more stable)
			var avgPrevCount int64
			err = p.db.QueryRow("SELECT COUNT(*) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
				dataEndTime.Add(-6*time.Minute).Unix(), dataEndTime.Add(-time.Minute).Unix()).Scan(&avgPrevCount)
			if err == nil && avgPrevCount > 0 {
				avgPrev := float64(avgPrevCount) / 5.0
				if avgPrev > 0 {
					percentageChange = ((float64(currentCount) - avgPrev) / avgPrev) * 100
				}
			}
			utils.LogInfo("POSTGRESQL_CHART", "ðŸ” %s Previous avg query (5min): %d records, change=%.1f%%", 
				period, avgPrevCount, percentageChange)
		} else if period == "1h" {
			// For hourly data, use previous hour comparison
			prevSince := dataEndTime.Add(-2*time.Hour)
			prevUntil := dataEndTime.Add(-time.Hour)
			err = p.db.QueryRow("SELECT COUNT(*) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
				prevSince.Unix(), prevUntil.Unix()).Scan(&prevCount)
			if err == nil && prevCount > 0 {
				percentageChange = ((float64(currentCount) - float64(prevCount)) / float64(prevCount)) * 100
			}
			utils.LogInfo("POSTGRESQL_CHART", "ðŸ” %s Previous period query: %s to %s = %d records, change=%.1f%%", 
				period, prevSince.Format("2006-01-02 15:04:05"), prevUntil.Format("2006-01-02 15:04:05"), prevCount, percentageChange)
		} else {
			// For daily and longer periods, use standard comparison
			prevSince := dataEndTime.Add(-2 * duration)
			prevUntil := dataEndTime.Add(-duration)
			err = p.db.QueryRow("SELECT COUNT(*) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
				prevSince.Unix(), prevUntil.Unix()).Scan(&prevCount)
			if err == nil && prevCount > 0 {
				percentageChange = ((float64(currentCount) - float64(prevCount)) / float64(prevCount)) * 100
			}
			utils.LogInfo("POSTGRESQL_CHART", "ðŸ” %s Previous period query: %s to %s = %d records, change=%.1f%%", 
				period, prevSince.Format("2006-01-02 15:04:05"), prevUntil.Format("2006-01-02 15:04:05"), prevCount, percentageChange)
		}
		
		// Default to 0% when no previous data (instead of 100%)
		if prevCount == 0 && period != "1m" {
			percentageChange = 0.0
		}
		
		utils.LogDebug("POSTGRESQL_CHART", "Period %s: current=%d, prev=%d, change=%.1f%%", 
			period, currentCount, prevCount, percentageChange)
		
		// Assign to appropriate field (original SQLite3 format)
		switch period {
		case "1m":
			rates.TxPerMinute = currentCount
			rates.PercentageChangeMin = percentageChange
			rates.TxPerMinuteChange = &percentageChange
		case "1h":
			rates.TxPerHour = currentCount
			rates.PercentageChangeHour = percentageChange
			rates.TxPerHourChange = &percentageChange
		case "1d":
			rates.TxPerDay = currentCount
			rates.PercentageChangeDay = percentageChange
			rates.TxPerDayChange = &percentageChange
		case "7d":
			rates.TxPer7Days = currentCount
			rates.PercentageChange7Day = percentageChange
			rates.TxPer7DaysChange = &percentageChange
		case "14d":
			rates.TxPer14Days = currentCount
			rates.PercentageChange14Day = percentageChange
			rates.TxPer14DaysChange = &percentageChange
		case "30d":
			rates.TxPer30Days = currentCount
			rates.PercentageChange30Day = percentageChange
			rates.TxPer30DaysChange = &percentageChange
		}
	}
	
	// Get unique senders and receivers totals from wallet stats
	var senders, receivers int64
	err = p.db.QueryRow(`
		SELECT unique_senders, unique_receivers 
		FROM wallet_stats 
		WHERE period = '30d' 
		ORDER BY calculated_at DESC 
		LIMIT 1`).Scan(&senders, &receivers)
	
	if err == nil {
		rates.UniqueSendersTotal = senders
		rates.UniqueReceiversTotal = receivers
		rates.TotalTracked = senders + receivers
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Transfer rates calculated using data timeline (ACCURATE)")
	return rates, nil
}

// GetActiveWalletRatesOptimized gets wallet statistics from materialized view (FAST)
// Replaces: 18+ expensive COUNT DISTINCT queries with 1 fast lookup
func (p *PostgreSQLChartService) GetActiveWalletRatesOptimized(rates FrontendTransferRates) map[string]interface{} {
	// Get actual data timeline for consistency
	var maxTimestamp int64
	p.db.QueryRow("SELECT MAX(timestamp) FROM transfers").Scan(&maxTimestamp)
	dataEndTime := time.Unix(maxTimestamp, 0)
	
	result := map[string]interface{}{
		"dataAvailable": true,
		"dataAvailability": DataAvailability{
			HasMinute: true,
			HasHour:   true,
			HasDay:    true,
			Has7Days:  true,
			Has14Days: true,
			Has30Days: true,
		},
		"serverUptime":         time.Since(dataEndTime.Add(-time.Hour)).Seconds(),
		"serverUptimeSeconds":  time.Since(dataEndTime.Add(-time.Hour)).Seconds(),
		"lastUpdateTime":       rates.LastUpdateTime,
		"uniqueSendersTotal":   rates.UniqueSendersTotal,
		"uniqueReceiversTotal": rates.UniqueReceiversTotal,
	}
	
	// Single fast query instead of 18+ expensive UNION/COUNT DISTINCT operations
	rows, err := p.db.Query(`
		SELECT period, unique_senders, unique_receivers, unique_total 
		FROM wallet_stats 
		WHERE period IN ('1m', '1h', '1d', '7d', '14d', '30d')
		ORDER BY 
			CASE period 
				WHEN '30d' THEN 1 
				WHEN '14d' THEN 2 
				WHEN '7d' THEN 3 
				WHEN '1d' THEN 4 
				WHEN '1h' THEN 5 
				WHEN '1m' THEN 6 
			END`)
	
	if err != nil {
		utils.LogError("POSTGRESQL_CHART", "Failed to query wallet stats: %v", err)
		return result
	}
	defer rows.Close()
	
	timeScaleMap := map[string]string{
		"1m": "LastMin", "1h": "LastHour", "1d": "LastDay",
		"7d": "Last7d", "14d": "Last14d", "30d": "Last30d",
	}
	
	durations := map[string]time.Duration{
		"1m": time.Minute, "1h": time.Hour, "1d": 24 * time.Hour,
		"7d": 7 * 24 * time.Hour, "14d": 14 * 24 * time.Hour, "30d": 30 * 24 * time.Hour,
	}
	
	// Use the dataEndTime from above
	
	for rows.Next() {
		var period string
		var senders, receivers, total int64
		if err = rows.Scan(&period, &senders, &receivers, &total); err != nil {
			continue
		}
		
		if periodName, ok := timeScaleMap[period]; ok {
			result["senders"+periodName] = senders
			result["receivers"+periodName] = receivers
			result["total"+periodName] = total
			
			// Calculate percentage changes using actual data timeline
			if duration, exists := durations[period]; exists {
				var prevSenders, prevReceivers int64
				var senderChange, receiverChange, totalChange float64
				
				// Use improved percentage calculation logic with data timeline
				if period == "1m" {
					// For minute data, use 5-minute average comparison
					var avgPrevSenders, avgPrevReceivers int64
					p.db.QueryRow("SELECT COUNT(DISTINCT sender) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
						dataEndTime.Add(-6*time.Minute).Unix(), dataEndTime.Add(-time.Minute).Unix()).Scan(&avgPrevSenders)
					p.db.QueryRow("SELECT COUNT(DISTINCT receiver) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
						dataEndTime.Add(-6*time.Minute).Unix(), dataEndTime.Add(-time.Minute).Unix()).Scan(&avgPrevReceivers)
					
					if avgPrevSenders > 0 {
						avgSenders := float64(avgPrevSenders) / 5.0
						if avgSenders > 0 {
							senderChange = ((float64(senders) - avgSenders) / avgSenders) * 100
						}
					}
					
					if avgPrevReceivers > 0 {
						avgReceivers := float64(avgPrevReceivers) / 5.0
						if avgReceivers > 0 {
							receiverChange = ((float64(receivers) - avgReceivers) / avgReceivers) * 100
						}
					}
					
					if avgPrevSenders > 0 && avgPrevReceivers > 0 {
						avgTotal := (float64(avgPrevSenders + avgPrevReceivers) / 5.0) * 0.7 // Account for overlap
						if avgTotal > 0 {
							totalChange = ((float64(total) - avgTotal) / avgTotal) * 100
						}
					}
				} else {
					// For longer periods, use data timeline for previous period
					prevSince := dataEndTime.Add(-2 * duration)
					prevUntil := dataEndTime.Add(-duration)
					
					// Get previous period counts
					p.db.QueryRow("SELECT COUNT(DISTINCT sender) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
						prevSince.Unix(), prevUntil.Unix()).Scan(&prevSenders)
					p.db.QueryRow("SELECT COUNT(DISTINCT receiver) FROM transfers WHERE timestamp > $1 AND timestamp <= $2", 
						prevSince.Unix(), prevUntil.Unix()).Scan(&prevReceivers)
					
					prevTotal := prevSenders + prevReceivers
					if prevSenders > 0 && prevReceivers > 0 {
						overlapEstimate := int64(float64(min(prevSenders, prevReceivers)) * 0.3)
						prevTotal -= overlapEstimate
					}
					
					// Only calculate if we have previous data
					if prevSenders > 0 {
						senderChange = ((float64(senders) - float64(prevSenders)) / float64(prevSenders)) * 100
					}
					
					if prevReceivers > 0 {
						receiverChange = ((float64(receivers) - float64(prevReceivers)) / float64(prevReceivers)) * 100
					}
					
					if prevTotal > 0 {
						totalChange = ((float64(total) - float64(prevTotal)) / float64(prevTotal)) * 100
					}
					
					utils.LogInfo("POSTGRESQL_CHART", "ðŸ“Š Wallet %s: senders=%d->%d (%.1f%%), receivers=%d->%d (%.1f%%), total=%d->%d (%.1f%%) [%s to %s]", 
						periodName, prevSenders, senders, senderChange, 
						prevReceivers, receivers, receiverChange,
						prevTotal, total, totalChange,
						prevSince.Format("2006-01-02 15:04:05"), prevUntil.Format("2006-01-02 15:04:05"))
				}
				
				// Default to 0% when no previous data
				if prevSenders == 0 && period != "1m" {
					senderChange = 0.0
				}
				if prevReceivers == 0 && period != "1m" {
					receiverChange = 0.0
				}
				
				result["senders"+periodName+"Change"] = senderChange
				result["receivers"+periodName+"Change"] = receiverChange
				result["total"+periodName+"Change"] = totalChange
			} else {
				// Fallback to zero if duration not found
				result["senders"+periodName+"Change"] = 0.0
				result["receivers"+periodName+"Change"] = 0.0
				result["total"+periodName+"Change"] = 0.0
			}
		}
		
		if period == "30d" {
			result["uniqueTotalWallets"] = total
		}
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Wallet rates calculated using data timeline (ACCURATE)")
	return result
}

// GetPopularRoutesOptimized gets popular routes from materialized view (FAST)
// Replaces: Expensive GROUP BY on millions of records
func (p *PostgreSQLChartService) GetPopularRoutesOptimized() ([]FrontendRouteData, error) {
	// Single fast query from pre-computed materialized view
	rows, err := p.db.Query(`
		SELECT source_chain, dest_chain, source_name, dest_name, 
		       route, transfer_count, last_activity
		FROM popular_routes 
		ORDER BY transfer_count DESC 
		LIMIT 10`)
	
	if err != nil {
		return nil, fmt.Errorf("failed to query popular routes: %w", err)
	}
	defer rows.Close()
	
	var routes []FrontendRouteData
	for rows.Next() {
		var route FrontendRouteData
		var lastActivity time.Time
		
		err := rows.Scan(&route.FromChain, &route.ToChain, &route.FromName, 
			&route.ToName, &route.Route, &route.Count, &lastActivity)
		if err != nil {
			continue
		}
		
		routes = append(routes, route)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Popular routes retrieved from materialized view (FAST)")
	return routes, nil
}

// GetChainFlowsOptimized gets chain flows from materialized view (FAST)
// Replaces: Expensive dual GROUP BY queries
func (p *PostgreSQLChartService) GetChainFlowsOptimized() ([]FrontendChainFlow, error) {
	// Single fast query from pre-computed materialized view
	rows, err := p.db.Query(`
		SELECT universal_chain_id, chain_name, outgoing_count, 
		       incoming_count, net_flow, last_activity
		FROM chain_flows 
		ORDER BY (outgoing_count + incoming_count) DESC 
		LIMIT 10`)
	
	if err != nil {
		return nil, fmt.Errorf("failed to query chain flows: %w", err)
	}
	defer rows.Close()
	
	var flows []FrontendChainFlow
	for rows.Next() {
		var flow FrontendChainFlow
		var lastActivity time.Time
		
		err := rows.Scan(&flow.UniversalChainID, &flow.ChainName, 
			&flow.OutgoingCount, &flow.IncomingCount, &flow.NetFlow, &lastActivity)
		if err != nil {
			continue
		}
		
		flow.LastActivity = lastActivity.Format(time.RFC3339)
		
		// Get top assets for this chain (could be materialized too)
		flow.TopAssets = p.getChainTopAssetsOptimized(flow.UniversalChainID)
		
		flows = append(flows, flow)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Chain flows retrieved from materialized view (FAST)")
	return flows, nil
}

// GetAssetVolumesOptimized gets asset volumes from materialized view (FAST)
// Replaces: Expensive token symbol GROUP BY queries
func (p *PostgreSQLChartService) GetAssetVolumesOptimized() ([]FrontendAsset, error) {
	// Single fast query from pre-computed materialized view
	rows, err := p.db.Query(`
		SELECT asset_symbol, asset_name, transfer_count, total_volume,
		       largest_transfer, average_amount, last_activity
		FROM asset_volumes 
		ORDER BY transfer_count DESC 
		LIMIT 10`)
	
	if err != nil {
		return nil, fmt.Errorf("failed to query asset volumes: %w", err)
	}
	defer rows.Close()
	
	var assets []FrontendAsset
	for rows.Next() {
		var asset FrontendAsset
		var lastActivity time.Time
		
		err := rows.Scan(&asset.AssetSymbol, &asset.AssetName, &asset.TransferCount,
			&asset.TotalVolume, &asset.LargestTransfer, &asset.AverageAmount, &lastActivity)
		if err != nil {
			continue
		}
		
		asset.LastActivity = lastActivity.Format(time.RFC3339)
		
		// Get top routes for this asset (could be materialized too)
		asset.TopRoutes = p.getAssetTopRoutesOptimized(asset.AssetSymbol)
		
		assets = append(assets, asset)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Asset volumes retrieved from materialized view (FAST)")
	return assets, nil
}

// GetActiveSendersOptimized gets active senders from materialized view (FAST)
func (p *PostgreSQLChartService) GetActiveSendersOptimized() ([]FrontendWalletData, error) {
	// Single fast query from pre-computed materialized view
	rows, err := p.db.Query(`
		SELECT address, display_address, count, last_activity
		FROM active_senders 
		ORDER BY count DESC 
		LIMIT 10`)
	
	if err != nil {
		return nil, fmt.Errorf("failed to query active senders: %w", err)
	}
	defer rows.Close()
	
	var wallets []FrontendWalletData
	for rows.Next() {
		var wallet FrontendWalletData
		var lastActivity time.Time
		
		err := rows.Scan(&wallet.Address, &wallet.DisplayAddress, &wallet.Count, &lastActivity)
		if err != nil {
			continue
		}
		
		// Format display address
		wallet.DisplayAddress = p.formatAddress(wallet.Address)
		wallet.LastActivity = lastActivity.Format(time.RFC3339)
		
		wallets = append(wallets, wallet)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Active senders retrieved from materialized view (FAST)")
	return wallets, nil
}

// GetActiveReceiversOptimized gets active receivers from materialized view (FAST)
func (p *PostgreSQLChartService) GetActiveReceiversOptimized() ([]FrontendWalletData, error) {
	// Single fast query from pre-computed materialized view
	rows, err := p.db.Query(`
		SELECT address, display_address, count, last_activity
		FROM active_receivers 
		ORDER BY count DESC 
		LIMIT 10`)
	
	if err != nil {
		return nil, fmt.Errorf("failed to query active receivers: %w", err)
	}
	defer rows.Close()
	
	var wallets []FrontendWalletData
	for rows.Next() {
		var wallet FrontendWalletData
		var lastActivity time.Time
		
		err := rows.Scan(&wallet.Address, &wallet.DisplayAddress, &wallet.Count, &lastActivity)
		if err != nil {
			continue
		}
		
		// Format display address
		wallet.DisplayAddress = p.formatAddress(wallet.Address)
		wallet.LastActivity = lastActivity.Format(time.RFC3339)
		
		wallets = append(wallets, wallet)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Active receivers retrieved from materialized view (FAST)")
	return wallets, nil
}

// GetChartDataOptimized gets all chart data using PostgreSQL optimizations (ULTRA FAST + ACCURATE)
// Uses: Materialized views when available, real-time calculations with proper timeline when needed
func (p *PostgreSQLChartService) GetChartDataOptimized() (map[string]interface{}, error) {
	utils.LogDebug("POSTGRESQL_CHART_SERVICE", "ðŸš€ Building ultra-fast chart data from materialized views")
	
	// Single query each to get pre-computed data
	transferRates, err := p.GetTransferRatesOptimized()
	if err != nil {
		utils.LogError("POSTGRESQL_CHART_SERVICE", "Failed to get transfer rates: %v", err)
		transferRates = FrontendTransferRates{
			DataAvailability: DataAvailability{
				HasMinute: false,
				HasHour:   false,
				HasDay:    false,
				Has7Days:  false,
				Has14Days: false,
				Has30Days: false,
			},
		}
	}
	
	activeWalletRates := p.GetActiveWalletRatesOptimized(transferRates)
	popularRoutes, _ := p.GetPopularRoutesOptimized()
	activeSenders, _ := p.GetActiveSendersOptimized()
	activeReceivers, _ := p.GetActiveReceiversOptimized()
	chainFlows, _ := p.GetChainFlowsOptimized()
	assetVolumes, _ := p.GetAssetVolumesOptimized()
	
	// Calculate totals from materialized view data
	totalOutgoing, totalIncoming := p.calculateTotalsFromViews(chainFlows)
	totalAssets, totalVolume, totalTransfers := p.calculateAssetTotalsFromViews(assetVolumes)
	
	// Build frontend-compatible response
	chartData := map[string]interface{}{
		"currentRates":     transferRates,
		"activeWalletRates": activeWalletRates,
		"popularRoutes":    popularRoutes,
		"popularRoutesTimeScale": map[string]interface{}{
			"1m": popularRoutes, // Use same data for all timeframes from materialized views
			"1h": popularRoutes,
			"1d": popularRoutes, 
			"7d": popularRoutes,
			"14d": popularRoutes,
			"30d": popularRoutes,
		},
		"activeSenders":   activeSenders,
		"activeSendersTimeScale": map[string]interface{}{
			"1m": activeSenders, // Use same data for all timeframes from materialized views
			"1h": activeSenders,
			"1d": activeSenders,
			"7d": activeSenders,
			"14d": activeSenders,
			"30d": activeSenders,
		},
		"activeReceivers": activeReceivers,
		"activeReceiversTimeScale": map[string]interface{}{
			"1m": activeReceivers, // Use same data for all timeframes from materialized views
			"1h": activeReceivers,
			"1d": activeReceivers,
			"7d": activeReceivers,
			"14d": activeReceivers,
			"30d": activeReceivers,
		},
		"chainFlowData": map[string]interface{}{
			"chains":             chainFlows,
			"chainFlowTimeScale": map[string]interface{}{
				"1m": chainFlows, // Use same data for all timeframes from materialized views
				"1h": chainFlows,
				"1d": chainFlows,
				"7d": chainFlows,
				"14d": chainFlows,
				"30d": chainFlows,
			},
			"totalOutgoing":      totalOutgoing,
			"totalIncoming":      totalIncoming,
			"serverUptimeSeconds": time.Since(time.Now().Add(-time.Hour)).Seconds(),
		},
		"assetVolumeData": map[string]interface{}{
			"assets":              assetVolumes,
			"assetVolumeTimeScale": map[string]interface{}{
				"1m": assetVolumes, // Use same data for all timeframes from materialized views
				"1h": assetVolumes,
				"1d": assetVolumes,
				"7d": assetVolumes,
				"14d": assetVolumes,
				"30d": assetVolumes,
			},
			"totalAssets":         totalAssets,
			"totalVolume":         totalVolume,
			"totalTransfers":      totalTransfers,
			"serverUptimeSeconds": time.Since(time.Now().Add(-time.Hour)).Seconds(),
		},
		"latencyData": p.getLatencyDataFromDB(), // Get actual latency data
		"nodeHealthData": p.getNodeHealthDataSummary(),
		// Add dataAvailability at the root level for charts to use
		"dataAvailability": map[string]interface{}{
			"hasMinute": p.hasDataInMaterializedViews(),
			"hasHour":   p.hasDataInMaterializedViews(),
			"hasDay":    p.hasDataInMaterializedViews(),
			"has7Days":  p.hasDataInMaterializedViews(),  
			"has14Days": p.hasDataInMaterializedViews(),
			"has30Days": p.hasDataInMaterializedViews(),
		},
		"timestamp": time.Now(),
	}
	
	utils.LogInfo("POSTGRESQL_CHART", "ðŸ“Š Ultra-fast chart data built from materialized views")
	return chartData, nil
}

// Helper functions

func (p *PostgreSQLChartService) getChainTopAssetsOptimized(chainID string) []FrontendChainAsset {
	// Query with proper outgoing/incoming separation and calculations
	rows, err := p.db.Query(`
		WITH chain_assets AS (
			-- Outgoing transfers from this chain
			SELECT 
				COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol) as symbol,
				COUNT(*) as outgoing_count,
				0::bigint as incoming_count,
				COALESCE(SUM(amount), 0) as outgoing_volume,
				0::numeric as incoming_volume,
				MAX(timestamp) as last_activity
			FROM transfers 
			WHERE source_chain = $1 
			  AND timestamp > NOW() - INTERVAL '30 days'
			  AND COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol) IS NOT NULL
			GROUP BY COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol)
			
			UNION ALL
			
			-- Incoming transfers to this chain
			SELECT 
				COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol) as symbol,
				0::bigint as outgoing_count,
				COUNT(*) as incoming_count,
				0::numeric as outgoing_volume,
				COALESCE(SUM(amount), 0) as incoming_volume,
				MAX(timestamp) as last_activity
			FROM transfers 
			WHERE dest_chain = $1 
			  AND timestamp > NOW() - INTERVAL '30 days'
			  AND COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol) IS NOT NULL
			GROUP BY COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol)
		)
		SELECT 
			symbol,
			SUM(outgoing_count) as total_outgoing,
			SUM(incoming_count) as total_incoming,
			SUM(outgoing_volume) as total_outgoing_volume,
			SUM(incoming_volume) as total_incoming_volume,
			MAX(last_activity) as last_activity
		FROM chain_assets
		GROUP BY symbol
		ORDER BY (SUM(outgoing_count) + SUM(incoming_count)) DESC
		LIMIT 5`, chainID)
	
	if err != nil {
		return []FrontendChainAsset{}
	}
	defer rows.Close()
	
	var assets []FrontendChainAsset
	var totalVolume float64 = 0
	
	// First pass - collect data and calculate total volume
	type assetData struct {
		asset  FrontendChainAsset
		volume float64
	}
	var assetList []assetData
	
	for rows.Next() {
		var asset FrontendChainAsset
		var lastActivity time.Time
		var outgoingVolume, incomingVolume float64
		
		err := rows.Scan(&asset.AssetSymbol, &asset.OutgoingCount, &asset.IncomingCount,
			&outgoingVolume, &incomingVolume, &lastActivity)
		if err != nil {
			continue
		}
		
		// Calculate derived fields
		asset.AssetName = asset.AssetSymbol
		asset.NetFlow = asset.IncomingCount - asset.OutgoingCount
		asset.TotalVolume = outgoingVolume + incomingVolume
		
		totalCount := asset.OutgoingCount + asset.IncomingCount
		if totalCount > 0 {
			asset.AverageAmount = asset.TotalVolume / float64(totalCount)
		} else {
			asset.AverageAmount = 0
		}
		
		asset.LastActivity = lastActivity.Format(time.RFC3339)
		
		assetList = append(assetList, assetData{asset: asset, volume: asset.TotalVolume})
		totalVolume += asset.TotalVolume
	}
	
	// Second pass - calculate percentages
	for _, ad := range assetList {
		asset := ad.asset
		if totalVolume > 0 {
			asset.Percentage = (ad.volume / totalVolume) * 100
		} else {
			asset.Percentage = 0
		}
		assets = append(assets, asset)
	}
	
	return assets
}

func (p *PostgreSQLChartService) getAssetTopRoutesOptimized(symbol string) []FrontendAssetRoute {
	// Query for top routes of this asset
	rows, err := p.db.Query(`
		SELECT source_chain, dest_chain, source_name, dest_name,
		       COUNT(*) as count, COALESCE(SUM(amount), 0) as volume,
		       MAX(timestamp) as last_activity
		FROM transfers 
		WHERE COALESCE(NULLIF(canonical_token_symbol, ''), token_symbol) = $1
		  AND timestamp > NOW() - INTERVAL '30 days'
		GROUP BY source_chain, dest_chain, source_name, dest_name
		ORDER BY count DESC 
		LIMIT 5`, symbol)
	
	if err != nil {
		return []FrontendAssetRoute{}
	}
	defer rows.Close()
	
	var routes []FrontendAssetRoute
	var totalVolume float64 = 0
	
	// First pass - collect data and calculate total volume
	type routeData struct {
		route  FrontendAssetRoute
		volume float64
	}
	var routeList []routeData
	
	for rows.Next() {
		var route FrontendAssetRoute
		var lastActivity time.Time
		var volume float64
		
		err := rows.Scan(&route.FromChain, &route.ToChain, &route.FromName, 
			&route.ToName, &route.Count, &volume, &lastActivity)
		if err != nil {
			continue
		}
		
		route.Route = fmt.Sprintf("%sâ†’%s", route.FromName, route.ToName)
		route.Volume = volume
		route.LastActivity = lastActivity.Format(time.RFC3339)
		
		routeList = append(routeList, routeData{route: route, volume: volume})
		totalVolume += volume
	}
	
	// Second pass - calculate percentages
	for _, rd := range routeList {
		route := rd.route
		if totalVolume > 0 {
			route.Percentage = (rd.volume / totalVolume) * 100
		} else {
			route.Percentage = 0
		}
		routes = append(routes, route)
	}
	
	return routes
}

func (p *PostgreSQLChartService) formatAddress(address string) string {
	if len(address) <= 16 {
		return address
	}
	return fmt.Sprintf("%s...%s", address[:8], address[len(address)-6:])
}

// RefreshMaterializedViews manually refreshes all materialized views
func (p *PostgreSQLChartService) RefreshMaterializedViews() error {
	_, err := p.db.Exec("SELECT refresh_analytics()")
	if err != nil {
		return fmt.Errorf("failed to refresh materialized views: %w", err)
	}
	
	utils.LogInfo("POSTGRESQL_CHART", "ðŸ”„ Materialized views refreshed manually")
	return nil
}

// GetMaterializedViewStatus returns the status of all materialized views
func (p *PostgreSQLChartService) GetMaterializedViewStatus() (map[string]interface{}, error) {
	rows, err := p.db.Query("SELECT * FROM get_analytics_status()")
	if err != nil {
		return nil, fmt.Errorf("failed to get analytics status: %w", err)
	}
	defer rows.Close()
	
	status := make(map[string]interface{})
	for rows.Next() {
		var viewName string
		var lastUpdated time.Time
		var recordCount int64
		
		if err := rows.Scan(&viewName, &lastUpdated, &recordCount); err != nil {
			continue
		}
		
		status[viewName] = map[string]interface{}{
			"last_updated":  lastUpdated.Format(time.RFC3339),
			"record_count":  recordCount,
			"age_seconds":   time.Since(lastUpdated).Seconds(),
		}
	}
	
	return status, nil
}

// hasDataInMaterializedViews checks if materialized views have data
func (p *PostgreSQLChartService) hasDataInMaterializedViews() bool {
	var count int64
	err := p.db.QueryRow("SELECT count FROM transfer_rates WHERE period = '30d' LIMIT 1").Scan(&count)
	if err != nil {
		return false
	}
	return count > 0
}

// Calculate totals from materialized view data
func (p *PostgreSQLChartService) calculateTotalsFromViews(chainFlows []FrontendChainFlow) (float64, float64) {
	var totalOutgoing, totalIncoming float64
	for _, flow := range chainFlows {
		totalOutgoing += float64(flow.OutgoingCount)
		totalIncoming += float64(flow.IncomingCount)
	}
	return totalOutgoing, totalIncoming
}

// Calculate asset totals from materialized view data
func (p *PostgreSQLChartService) calculateAssetTotalsFromViews(assets []FrontendAsset) (float64, float64, float64) {
	var totalAssets, totalVolume, totalTransfers float64
	for _, asset := range assets {
		totalAssets += 1
		totalVolume += asset.TotalVolume
		totalTransfers += float64(asset.TransferCount)
	}
	return totalAssets, totalVolume, totalTransfers
}

// getLatencyDataFromDB retrieves the latest latency data from database
func (p *PostgreSQLChartService) getLatencyDataFromDB() []interface{} {
	query := `
		SELECT DISTINCT 
			l1.source_chain, l1.dest_chain, l1.source_name, l1.dest_name,
			l1.packet_ack_p5, l1.packet_ack_median, l1.packet_ack_p95,
			l1.packet_recv_p5, l1.packet_recv_median, l1.packet_recv_p95,
			l1.write_ack_p5, l1.write_ack_median, l1.write_ack_p95
		FROM latency_data l1
		INNER JOIN (
			SELECT source_chain, dest_chain, MAX(fetched_at) as max_fetched
			FROM latency_data 
			GROUP BY source_chain, dest_chain
		) l2 ON l1.source_chain = l2.source_chain 
			AND l1.dest_chain = l2.dest_chain 
			AND l1.fetched_at = l2.max_fetched
		ORDER BY l1.source_name, l1.dest_name`
	
	rows, err := p.db.Query(query)
	if err != nil {
		utils.LogError("POSTGRESQL_CHART", "Failed to query latency data: %v", err)
		return []interface{}{}
	}
	defer rows.Close()
	
	var latencyData []interface{}
	for rows.Next() {
		var sourceChain, destChain, sourceName, destName string
		var packetAckP5, packetAckMedian, packetAckP95 float64
		var packetRecvP5, packetRecvMedian, packetRecvP95 float64
		var writeAckP5, writeAckMedian, writeAckP95 float64
		
		err := rows.Scan(
			&sourceChain, &destChain, &sourceName, &destName,
			&packetAckP5, &packetAckMedian, &packetAckP95,
			&packetRecvP5, &packetRecvMedian, &packetRecvP95,
			&writeAckP5, &writeAckMedian, &writeAckP95,
		)
		
		if err != nil {
			utils.LogError("POSTGRESQL_CHART", "Failed to scan latency row: %v", err)
			continue
		}
		
		latency := map[string]interface{}{
			"sourceChain":      sourceChain,
			"destinationChain": destChain,
			"sourceName":       sourceName,
			"destinationName":  destName,
			"packetAck": map[string]interface{}{
				"p5":     packetAckP5,
				"median": packetAckMedian,
				"p95":    packetAckP95,
			},
			"packetRecv": map[string]interface{}{
				"p5":     packetRecvP5,
				"median": packetRecvMedian,
				"p95":    packetRecvP95,
			},
			"writeAck": map[string]interface{}{
				"p5":     writeAckP5,
				"median": writeAckMedian,
				"p95":    writeAckP95,
			},
		}
		
		latencyData = append(latencyData, latency)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Retrieved %d latency records from database", len(latencyData))
	return latencyData
}

// getNodeHealthDataSummary retrieves node health data from database
func (p *PostgreSQLChartService) getNodeHealthDataSummary() map[string]interface{} {
	// Get latest health data
	query := `
		SELECT 
			nh1.chain_id, nh1.chain_name, nh1.rpc_url, nh1.rpc_type, 
			nh1.status, nh1.response_time_ms, nh1.latest_block_height, 
			nh1.error_message, nh1.uptime, nh1.checked_at
		FROM node_health nh1
		INNER JOIN (
			SELECT rpc_url, MAX(checked_at) as max_checked
			FROM node_health 
			WHERE checked_at > NOW() - INTERVAL '1 hour'
			GROUP BY rpc_url
		) nh2 ON nh1.rpc_url = nh2.rpc_url 
			AND nh1.checked_at = nh2.max_checked
		ORDER BY nh1.chain_name, nh1.rpc_url`
	
	rows, err := p.db.Query(query)
	if err != nil {
		utils.LogError("POSTGRESQL_CHART", "Failed to query node health data: %v", err)
		return map[string]interface{}{
			"totalNodes":       0,
			"healthyNodes":     0,
			"degradedNodes":    0,
			"unhealthyNodes":   0,
			"avgResponseTime":  0.0,
			"nodesWithRpcs":    []interface{}{},
			"chainHealthStats": map[string]interface{}{},
			"dataAvailability": map[string]interface{}{
				"hasMinute": false,
				"hasHour":   false,
				"hasDay":    false,
				"has7Days":  false,
				"has14Days": false,
				"has30Days": false,
			},
		}
	}
	defer rows.Close()
	
	var nodes []interface{}
	totalNodes := 0
	healthyNodes := 0
	degradedNodes := 0
	unhealthyNodes := 0
	totalResponseTime := 0
	responseTimeCount := 0
	chainStats := make(map[string]map[string]interface{})
	
	for rows.Next() {
		var chainID, chainName, rpcURL, rpcType, status, errorMessage string
		var responseTimeMs int
		var blockHeight *int64
		var uptime float64
		var checkedAt time.Time
		
		err := rows.Scan(&chainID, &chainName, &rpcURL, &rpcType, &status, 
			&responseTimeMs, &blockHeight, &errorMessage, &uptime, &checkedAt)
		if err != nil {
			utils.LogError("POSTGRESQL_CHART", "Failed to scan node health row: %v", err)
			continue
		}
		
		totalNodes++
		
		// Count by status
		switch status {
		case "healthy":
			healthyNodes++
			if responseTimeMs > 0 {
				totalResponseTime += responseTimeMs
				responseTimeCount++
			}
		case "degraded":
			degradedNodes++
			if responseTimeMs > 0 {
				totalResponseTime += responseTimeMs
				responseTimeCount++
			}
		case "unhealthy":
			unhealthyNodes++
		}
		
		// Build node info
		nodeInfo := map[string]interface{}{
			"chainId":      chainID,
			"chainName":    chainName,
			"rpcUrl":       rpcURL,
			"rpcType":      rpcType,
			"status":       status,
			"responseTime": responseTimeMs,
			"blockHeight":  blockHeight,
			"uptime":       uptime,
			"lastCheck":    checkedAt.Unix(),
		}
		if errorMessage != "" {
			nodeInfo["error"] = errorMessage
		}
		nodes = append(nodes, nodeInfo)
		
		// Aggregate chain stats
		if _, exists := chainStats[chainID]; !exists {
			chainStats[chainID] = map[string]interface{}{
				"chainName":      chainName,
				"totalNodes":     0,
				"healthyNodes":   0,
				"degradedNodes":  0,
				"unhealthyNodes": 0,
				"healthScore":    0.0,
			}
		}
		
		stat := chainStats[chainID]
		stat["totalNodes"] = stat["totalNodes"].(int) + 1
		
		switch status {
		case "healthy":
			stat["healthyNodes"] = stat["healthyNodes"].(int) + 1
		case "degraded":
			stat["degradedNodes"] = stat["degradedNodes"].(int) + 1
		case "unhealthy":
			stat["unhealthyNodes"] = stat["unhealthyNodes"].(int) + 1
		}
		
		// Update health score
		total := stat["totalNodes"].(int)
		healthy := stat["healthyNodes"].(int)
		if total > 0 {
			stat["healthScore"] = float64(healthy) / float64(total) * 100
		}
	}
	
	// Calculate average response time
	avgResponseTime := 0.0
	if responseTimeCount > 0 {
		avgResponseTime = float64(totalResponseTime) / float64(responseTimeCount)
	}
	
	utils.LogDebug("POSTGRESQL_CHART", "Retrieved node health summary: %d nodes (%d healthy, %d degraded, %d unhealthy)", 
		totalNodes, healthyNodes, degradedNodes, unhealthyNodes)
	
	return map[string]interface{}{
		"totalNodes":       totalNodes,
		"healthyNodes":     healthyNodes,
		"degradedNodes":    degradedNodes,
		"unhealthyNodes":   unhealthyNodes,
		"avgResponseTime":  avgResponseTime,
		"nodesWithRpcs":    nodes,
		"chainHealthStats": chainStats,
		"dataAvailability": map[string]interface{}{
			"hasMinute": totalNodes > 0,
			"hasHour":   totalNodes > 0,
			"hasDay":    p.hasNodeHealthDataForPeriod(24 * time.Hour),
			"has7Days":  p.hasNodeHealthDataForPeriod(7 * 24 * time.Hour),
			"has14Days": p.hasNodeHealthDataForPeriod(14 * 24 * time.Hour),
			"has30Days": p.hasNodeHealthDataForPeriod(30 * 24 * time.Hour),
		},
	}
}

// hasNodeHealthDataForPeriod checks if we have node health data for a given period
func (p *PostgreSQLChartService) hasNodeHealthDataForPeriod(period time.Duration) bool {
	query := `SELECT COUNT(*) FROM node_health WHERE checked_at > $1`
	cutoff := time.Now().Add(-period)
	
	var count int
	err := p.db.QueryRow(query, cutoff).Scan(&count)
	if err != nil {
		return false
	}
	return count > 0
}

// LoadLatencyDataFromDB - No-op for PostgreSQL service (always uses fresh data)
func (p *PostgreSQLChartService) LoadLatencyDataFromDB() error {
	utils.LogDebug("POSTGRESQL_CHART", "LoadLatencyDataFromDB: PostgreSQL service always uses fresh data")
	return nil
}

// RefreshCache - No-op for PostgreSQL service (always fresh, no cache needed)
func (p *PostgreSQLChartService) RefreshCache() error {
	utils.LogDebug("POSTGRESQL_CHART", "RefreshCache: PostgreSQL service is always fresh, no cache to refresh")
	return nil
}

// SetLatencyData - Store latency data (called by health callback)
func (p *PostgreSQLChartService) SetLatencyData(data []models.LatencyData) {
	utils.LogDebug("POSTGRESQL_CHART", "SetLatencyData: Received %d latency records", len(data))
	// The latency data is automatically stored by the health service
	// PostgreSQL service reads directly from database, so no need to cache
}

// SetNodeHealthData - Store node health data (called by health callback)  
func (p *PostgreSQLChartService) SetNodeHealthData(data []models.NodeHealthData) {
	utils.LogDebug("POSTGRESQL_CHART", "SetNodeHealthData: Received %d node health records", len(data))
	// The node health data is automatically stored by the health service
	// PostgreSQL service reads directly from database, so no need to cache
}

// GetChartDataForFrontend - Main method for getting chart data (optimized)
func (p *PostgreSQLChartService) GetChartDataForFrontend() (map[string]interface{}, error) {
	return p.GetChartDataOptimized()
}

// GetCacheStats - Returns cache statistics (PostgreSQL service always fresh, no cache)
func (p *PostgreSQLChartService) GetCacheStats() map[string]interface{} {
	return map[string]interface{}{
		"service_type":        "postgresql_optimized",
		"cache_enabled":       false,
		"always_fresh":        true,
		"uses_materialized_views": true,
		"description":         "PostgreSQL service uses materialized views and real-time queries - no cache needed",
		"query_speed":         "ultra_fast",
		"data_freshness":      "real_time",
		"cache_size":          0,
		"cache_hit_rate":      "N/A - no cache",
		"last_refresh":        "always",
		"refresh_strategy":    "materialized_views_with_real_time_fallback",
	}
}

 